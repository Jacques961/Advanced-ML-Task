# -*- coding: utf-8 -*-
"""SpamClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zqln3G56gRXf3nLPRDRRpGMbZeiHhKY8

# SMS Spam Classification (Text Classification)

# Load and explore the datasets

## Load the datasets
"""

# Commented out IPython magic to ensure Python compatibility.
# %reset -f
import pandas as pd

data_frame_1 = pd.read_csv('/content/spam.csv')
data_frame_2 = pd.read_csv('/content/emails.csv')

print('SMS data shape:')
print(data_frame_1.shape)

print('\nEmails data shape:')
print(data_frame_2.shape)

"""## Explore columns and first few rows"""

print('Columns in SMS data:')
print(data_frame_1.columns)

print('\nData head of SMS data:')
print(data_frame_1.head())

print('\nColumns in Emails data:')
print(data_frame_2.columns)

print('\nData head of Email data:')
print(data_frame_2.head())

"""## Check null values count in datatsets"""

print('\nNull values count in SMS data:')
print(data_frame_1.isna().sum())

print('\nNull values count in Emails data:')
print(data_frame_2.isna().sum())

"""## Datatypes in datasets"""

print('\nData types in SMS data:')
print(data_frame_1.dtypes)

print('\nData types in Emails data:')
print(data_frame_1.dtypes)

"""## Generate an automatic Explanatory Data Analysis EDA report"""

!pip install ydata_profiling

from ydata_profiling import ProfileReport
profile_data_frame_1 = ProfileReport(data_frame_1, title="SMS Data Profiling Report")
profile_data_frame_2 = ProfileReport(data_frame_2, title="Emails Data Profiling Report")

profile_data_frame_1.to_file("sms_data_report.html")
print('EDA report created for SMS dataset\n\n\n')

profile_data_frame_2.to_file("emails_data_report.html")
print('EDA report created for Emails dataset')

"""# Datasets Preprocessing, Cleaning and Resampling

## Drop duplicate rows from datasets
"""

print('There are', data_frame_1.duplicated().sum(),'duplicate rows in SMS dataset')
print('There are', data_frame_2.duplicated().sum(),'duplicate rows in Emails dataset')

# remove duplicates from data_frame_1
data_frame_1 = data_frame_1.drop_duplicates().copy()
print('\nDuplicate rows in SMS data are:', data_frame_1.duplicated().sum())

# remove duplicated from data_frame_2
data_frame_2 = data_frame_2.drop_duplicates().copy()
print('Duplicate rows in Emails data are:', data_frame_2.duplicated().sum())

"""## Check data distribution in datasets"""

print('SMS data distribution:')
print(data_frame_1['Category'].value_counts())
data_frame_1_imbalance_ratio = data_frame_1['Category'].value_counts().iloc[0]/ data_frame_1['Category'].value_counts().iloc[1]
print('Imbalanace ratio:', data_frame_1_imbalance_ratio)

print('\nEmails data distribution:')
data_frame_2_imbalance_ratio = data_frame_2['spam'].value_counts().iloc[0] / data_frame_2['spam'].value_counts().iloc[1]
print(data_frame_2['spam'].value_counts())
print('Imbalance ratio:', data_frame_2_imbalance_ratio)

"""## Plot data distribution"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: SMS Scatter plot
sns.countplot(data=data_frame_1, x='Category', hue='Category', ax=axes[0])
axes[0].set_title('Spam/Not Spam Distribution in SMS')
axes[0].legend(title='Spam')

# Plot 2: Email Data
sns.countplot(data=data_frame_2, x='spam', hue='spam', ax=axes[1])
axes[1].set_title('Spam/Not Spam Distribution in Emails')
axes[1].legend(title='Spam')

plt.tight_layout()
plt.show()

"""***Data distribution for both classes is imbalanced so we need to do resampling for the data***

## Rename columns as 'text' and 'label'
"""

data_frame_1.rename(columns={'Category': 'label', 'Message': 'text'}, inplace=True)
data_frame_2.rename(columns={'spam': 'label', 'text':'text'}, inplace=True)

"""## Map label values to 0 for not spam and 1 for spam in SMS data"""

data_frame_1['label'] = data_frame_1['label'].map({'ham': 0, 'spam': 1})

"""## Add length column for datasets"""

data_frame_1['length'] = data_frame_1['text'].apply(lambda x: len(x))
print(data_frame_1.head())

data_frame_2['length'] = data_frame_2['text'].apply(lambda x: len(x))
print(data_frame_2.head())

"""## Pipeline for cleaning text raw data"""

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('stopwords')
import re
import string

# remove the 'Subject' from text in Emails data
def remove_subject_from_text_emails(text: str):
    if text.lower().startswith("subject:"):
        return text[len("subject:"):].strip()
    return text

# to lower
def to_lower(text: str):
  return text.lower()

# remove numbers from text
def remove_numbers(text: str):
  for c in text:
    if c.isnumeric():
      text = text.replace(c,' ')
  return text

# lemmatizing
def lemmatizing(text: str):
  lemmatizer = WordNetLemmatizer()
  tokens = word_tokenize(text)
  for i in range(len(tokens)):
    lemma_world = lemmatizer.lemmatize(tokens[i])
    tokens[i] = lemma_world
  return " ".join(tokens)

# remove punctuation
def remove_punctuation(text: str):
  for c in text:
    if c in string.punctuation:
      text = text.replace(c,'')
  return text

# remove stop words
def remove_stopwords(text: str):
  removed = []
  stop_words = list(stopwords.words("english"))
  tokens = word_tokenize(text)
  for i in range(len(tokens)):
    if tokens[i] not in stop_words:
      removed.append(tokens[i])
  return " ".join(removed)

# remove extra white spaces
def remove_extra_white_spaces(text: str):
  single_char_pattern = r'\s+[a-zA-Z]\s+'
  without_sc = re.sub(pattern=single_char_pattern, repl=" ", string= text)
  return without_sc

data_frame_2['text'] = data_frame_2['text'].apply(lambda x: remove_subject_from_text_emails(x))

data_frame_1['text'] = data_frame_1['text'].apply(lambda x: to_lower(x))
data_frame_2['text'] = data_frame_2['text'].apply(lambda x: to_lower(x))

data_frame_1['text'] = data_frame_1['text'].apply(lambda x: remove_numbers(x))
data_frame_2['text'] = data_frame_2['text'].apply(lambda x: remove_numbers(x))

data_frame_1['text'] = data_frame_1['text'].apply(lambda x: remove_punctuation(x))
data_frame_2['text'] = data_frame_2['text'].apply(lambda x: remove_punctuation(x))

data_frame_1['text'] = data_frame_1['text'].apply(lambda x: remove_stopwords(x))
data_frame_2['text'] = data_frame_2['text'].apply(lambda x: remove_stopwords(x))

data_frame_1['text'] = data_frame_1['text'].apply(lambda x: remove_extra_white_spaces(x))
data_frame_2['text'] = data_frame_2['text'].apply(lambda x: remove_extra_white_spaces(x))

data_frame_1['text'] = data_frame_1['text'].apply(lambda x: lemmatizing(x))
data_frame_2['text'] = data_frame_2['text'].apply(lambda x: lemmatizing(x))

"""## Update the length after cleaning"""

data_frame_1['length_after_cleaning'] = data_frame_1['text'].apply(lambda x: len(x)).copy()
data_frame_2['length_after_cleaning'] = data_frame_2['text'].apply(lambda x:len(x)).copy()

print('After cleaning SMS data:')
display(data_frame_1.head())

print('\nAfter cleaning Emails data:')
display(data_frame_2.head())

"""## Better to split now the data into training and testing to avoid data leakage"""

from sklearn.model_selection import train_test_split
# split SMS data
X_train_SMS, X_test_SMS, y_train_SMS, y_test_SMS = train_test_split(data_frame_1['text'], data_frame_1['label'], test_size = 0.25,
                                                                    stratify = data_frame_1['label'], random_state=42)
print('Distribution in SMS train data: ')
display(y_train_SMS.value_counts())

# split Emails data
X_train_Emails, X_test_Emails, y_train_Emails, y_test_Emails = train_test_split(data_frame_2['text'], data_frame_2['label'], test_size = 0.25,
                                                                                stratify = data_frame_2['label'], random_state=42)
print('\nDistribution in Emails train data: ')
display(y_train_Emails.value_counts())

"""## Augment SMS training data using NLPAUG"""

!pip install nlpaug

import nlpaug.augmenter.word.context_word_embs as aug
from tqdm import tqdm

# make a data frame from the training data of SMS data
data_frame_1_training = pd.DataFrame({'text': X_train_SMS, 'label': y_train_SMS})

print('Distribution before augmentation:')
display(data_frame_1_training['label'].value_counts())

augmenter = aug.ContextualWordEmbsAug(
    model_path='distilbert-base-uncased',
    action="insert",
    top_k=20)

augmented_rows = []

# augment the entries having label as 1 in the sample_data_frame_1 and append it
spam_df = data_frame_1_training[data_frame_1_training['label'] == 1].reset_index(drop=True)

for i in tqdm(range(len(spam_df)), desc="Augmenting spam messages"):
    for _ in range(3):
        augmented = augmenter.augment(spam_df.iloc[i]['text'])
        if isinstance(augmented, list):
            augmented_text = " ".join(augmented)
        else:
            augmented_text = augmented
        augmented_rows.append({'text': augmented_text, 'label': 1})

aug_df = pd.DataFrame(augmented_rows)
data_frame_1_training_augmented = pd.concat([data_frame_1_training, aug_df], ignore_index=True)

print('\nDistribution after augmentation:')
display(data_frame_1_training_augmented['label'].value_counts())

""" ## Set training SMS data"""

X_train_SMS_balanced = data_frame_1_training_augmented['text']
y_train_SMS_balanced = data_frame_1_training_augmented['label']

"""## Save a raw version of SMS augmented data"""

X_train_SMS_balanced_raw = X_train_SMS_balanced.copy()
y_train_SMS_balanced_raw = y_train_SMS_balanced.copy()

X_test_SMS_raw = X_test_SMS.copy()

"""## Vectorize text in SMS data"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer_SMS = TfidfVectorizer()
X_train_SMS_balanced = vectorizer_SMS.fit_transform(X_train_SMS_balanced)
X_test_SMS = vectorizer_SMS.transform(X_test_SMS)

"""## Augment Emails data using SMOTE"""

from imblearn.over_sampling import SMOTE

# vectorize the text before applying SMOTE
vectorizer_Emails = TfidfVectorizer()
X_train_Emails_vectorized = vectorizer_Emails.fit_transform(X_train_Emails)

# apply SMOTE
smote = SMOTE(random_state=42)
X_train_Emails_balanced, y_train_Emails_balanced = smote.fit_resample(X_train_Emails_vectorized, y_train_Emails)

print('Distribution before augmentation:')
display(y_train_Emails.value_counts())

print('\nDistribution after augmentation:')
display(y_train_Emails_balanced.value_counts())

"""## Vectorize text in test Emails data"""

X_test_Emails = vectorizer_Emails.transform(X_test_Emails)

"""# Models"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
import sklearn.metrics
import numpy as np

# define the parameter grid
param_grid = [
    # Most commonly used: L2 penalty with reliable solvers
    {
        'penalty': ['l2'],
        'C': [0.01, 0.1, 1, 10],
        'solver': ['liblinear', 'lbfgs'],
        'max_iter': [100, 1000]
    },
    # L1 penalty: only use liblinear for speed and compatibility
    {
        'penalty': ['l1'],
        'C': [0.01, 0.1, 1],
        'solver': ['liblinear'],
        'max_iter': [1000]
    }
]

"""### SMS Logistic Regression Model"""

logistic_regression_model_SMS = GridSearchCV(LogisticRegression(), param_grid, cv=4, scoring='f1', n_jobs=-1, verbose=1)

print('Logistic Regression Model on SMS Dataset:')
logistic_regression_model_SMS.fit(X_train_SMS_balanced, y_train_SMS_balanced)

print('\nTraining accuracy:')
y_train_pred = logistic_regression_model_SMS.predict(X_train_SMS_balanced)
print(sklearn.metrics.accuracy_score(y_train_SMS_balanced, y_train_pred))

print('\nBest parameters:')
print(logistic_regression_model_SMS.best_params_)

print('\nBest estimator:')
print(logistic_regression_model_SMS.best_estimator_)

print('\nBest score:')
print(logistic_regression_model_SMS.best_score_)

# testing the model

predictSMS = logistic_regression_model_SMS.predict(X_test_SMS)

print('\nTesting accuracy:')
print(sklearn.metrics.accuracy_score(y_test_SMS, predictSMS))

print('\nConfusion matrix:')
print(confusion_matrix(y_test_SMS, predictSMS))

cm1 = confusion_matrix(y_test_SMS, predictSMS)
sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print('\nClassification report:')
print(classification_report(y_test_SMS, predictSMS))

"""#### Investigate misclassified SMS messages"""

misclassified_indices = np.where(predictSMS != y_test_SMS)[0]
print(f"\nNumber of misclassified SMS: {len(misclassified_indices)}")

error_df = pd.DataFrame({
    'text': X_test_SMS_raw.iloc[misclassified_indices],  # this is the cleaned version of test data
    'true_label': y_test_SMS.iloc[misclassified_indices].values,
    'predicted': predictSMS[misclassified_indices]
})

print("\nSample misclassified SMS messages:")
display(error_df.head(5))

"""### Emails Logistic Regression Model"""

logistic_regression_model_Emails = GridSearchCV(LogisticRegression(), param_grid, cv=4, scoring='f1', n_jobs=-1, verbose=1)

print('Logistic Regression Model on Emails Dataset:')
logistic_regression_model_Emails.fit(X_train_Emails_balanced, y_train_Emails_balanced)

print('\nTraining accuracy:')
y_train_pred = logistic_regression_model_Emails.predict(X_train_Emails_balanced)
print(sklearn.metrics.accuracy_score(y_train_Emails_balanced, y_train_pred))

print('\nBest parameters:')
print(logistic_regression_model_Emails.best_params_)

print('\nBest estimator:')
print(logistic_regression_model_Emails.best_estimator_)

print('\nBest score:')
print(logistic_regression_model_Emails.best_score_)

# testing the model

predictEmails = logistic_regression_model_Emails.predict(X_test_Emails)

print('\nTesting accuracy:')
print(sklearn.metrics.accuracy_score(y_test_Emails, predictEmails))

print('\nConfusion matrix:')
print(confusion_matrix(y_test_Emails, predictEmails))

cm1 = confusion_matrix(y_test_Emails, predictEmails)
sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print('\nClassification report:')
print(classification_report(y_test_Emails, predictEmails))

"""## Multinomial"""

from sklearn.naive_bayes import MultinomialNB

"""### SMS Multinomial Model"""

param_grid_nb = {'alpha': [0.1, 0.5, 1.0, 2.0]}
grid_nb_SMS = GridSearchCV(MultinomialNB(), param_grid_nb, cv=4, scoring='f1', n_jobs=-1)
multinomial_model_SMS = MultinomialNB()

print('Multinomial Model on SMS Dataset:')
multinomial_model_SMS.fit(X_train_SMS_balanced, y_train_SMS_balanced)

print('\nTraining accuracy:')
y_train_pred = multinomial_model_SMS.predict(X_train_SMS_balanced)
print(sklearn.metrics.accuracy_score(y_train_SMS_balanced, y_train_pred))

# testing the model

predictSMS = multinomial_model_SMS.predict(X_test_SMS)

print('\nTesting accuracy:')
print(sklearn.metrics.accuracy_score(y_test_SMS, predictSMS))

print('\nConfusion matrix:')
print(confusion_matrix(y_test_SMS, predictSMS))

cm1 = confusion_matrix(y_test_SMS, predictSMS)
sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Multinomial Model Confusion Matrix on SMS')
plt.show()

print('\nClassification report:')
print(classification_report(y_test_SMS, predictSMS))

"""### Emails Multinomial Model"""

multinomial_model_Emails = GridSearchCV(MultinomialNB(), param_grid_nb, cv=4, scoring='f1', n_jobs=-1)

print('Multinomial Model on Emails Dataset:')
multinomial_model_Emails.fit(X_train_Emails_balanced, y_train_Emails_balanced)

print('\nTraining accuracy:')
y_train_pred = multinomial_model_Emails.predict(X_train_Emails_balanced)
print(sklearn.metrics.accuracy_score(y_train_Emails_balanced, y_train_pred))

# testing the model

predictEmails = multinomial_model_Emails.predict(X_test_Emails)

print('\nTesting accuracy:')
print(sklearn.metrics.accuracy_score(y_test_Emails, predictEmails))

print('\nConfusion matrix:')
print(confusion_matrix(y_test_Emails, predictEmails))

cm1 = confusion_matrix(y_test_Emails, predictEmails)
sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Multinomial Model Confusion Matrix on Emails')
plt.show()

print('\nClassification report:')
print(classification_report(y_test_Emails, predictEmails))

"""## Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

"""### SMS Random Forest Classifier Model"""

RFC_model_SMS = RandomForestClassifier(n_estimators=50, random_state=42)

print('RFC Model on SMS Dataset:')
RFC_model_SMS.fit(X_train_SMS_balanced, y_train_SMS_balanced)

print('\nTraining accuracy:')
y_train_pred = RFC_model_SMS.predict(X_train_SMS_balanced)
print(sklearn.metrics.accuracy_score(y_train_SMS_balanced, y_train_pred))

# testing the model

predictSMS = RFC_model_SMS.predict(X_test_SMS)

print('\nTesting accuracy:')
print(sklearn.metrics.accuracy_score(y_test_SMS, predictSMS))

print('\nConfusion matrix:')
print(confusion_matrix(y_test_SMS, predictSMS))

cm1 = confusion_matrix(y_test_SMS, predictSMS)
sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('RFC Model Confusion Matrix on SMS')
plt.show()

print('\nClassification report:')
print(classification_report(y_test_SMS, predictSMS))

"""### Emails Random Forest Classifier Model"""

RFC_model_Emails = RandomForestClassifier(n_estimators=50, random_state=42)

print('RFC Model on Emails Dataset:')
RFC_model_Emails.fit(X_train_Emails_balanced, y_train_Emails_balanced)

print('\nTraining accuracy:')
y_train_pred = RFC_model_Emails.predict(X_train_Emails_balanced)
print(sklearn.metrics.accuracy_score(y_train_Emails_balanced, y_train_pred))

# testing the model

predictEmails = RFC_model_Emails.predict(X_test_Emails)

print('\nTesting accuracy:')
print(sklearn.metrics.accuracy_score(y_test_Emails, predictEmails))

print('\nConfusion matrix:')
print(confusion_matrix(y_test_Emails, predictEmails))

cm1 = confusion_matrix(y_test_Emails, predictEmails)
sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('RFC Model Confusion Matrix on Emails')
plt.show()

print('\nClassification report:')
print(classification_report(y_test_Emails, predictEmails))